{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "547YdGaZRdoj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2057
        },
        "outputId": "0e12a79a-fc02-45c8-d87e-77f0fa00a6ce"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "NUM_CLASSES = 10\n",
        "LEARNING_RATE = 0.001\n",
        "IMG_DIM = 28*28\n",
        "NUM_EPOCHS = 40\n",
        "\n",
        "# Prepare data given a numpy array of \n",
        "def prepDataset(ds):\n",
        "  d = tf.data.Dataset.from_tensor_slices(ds)\n",
        "  d = d.map(lambda x, y: (tf.cast(tf.reshape(x, [IMG_DIM]), tf.float32), tf.cast(y, tf.int32)))\n",
        "  d = d.batch(BATCH_SIZE)\n",
        "  iter = d.make_initializable_iterator()\n",
        "  return iter, iter.get_next()\n",
        "\n",
        "\n",
        "# Evaluate model with test data\n",
        "# sess: session runtime\n",
        "def evalTestData(sess):\n",
        "  sess.run(test_iter.initializer)\n",
        "  tc, steps = 0, 0\n",
        "  while True:\n",
        "    try:\n",
        "      test_x, test_y = sess.run((test_nx, test_ny))\n",
        "      c, l = sess.run((correct, loss), feed_dict={x: test_x, y: test_y})\n",
        "      losses.append(l)\n",
        "      tc += c\n",
        "      steps += 1\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      avg_loss = sum(losses) / len(losses)\n",
        "      print(\"TEST: batch size: %d, steps %d, avg loss %f, precision: %f\" % (BATCH_SIZE, steps, avg_loss, tc/(steps*BATCH_SIZE)*100))\n",
        "      break\n",
        "\n",
        "# Load data using keras.\n",
        "(train_iter, (train_nx, train_ny)), (test_iter, (test_nx, test_ny)) = map(prepDataset, tf.keras.datasets.mnist.load_data())\n",
        "\n",
        "# DEFINE MODEL, building graph\n",
        "x, y = tf.placeholder(tf.float32, shape=(BATCH_SIZE, IMG_DIM)), tf.placeholder(tf.int32, shape=(BATCH_SIZE))\n",
        "# first hidden layer\n",
        "h1_layer = tf.layers.Dense(units=128, activation=tf.nn.relu)\n",
        "h1 = h1_layer(x)\n",
        "# second hidden layer\n",
        "h2_layer = tf.layers.Dense(units=32, activation=tf.nn.relu)\n",
        "h2 = h2_layer(h1)\n",
        "# softmax/logit layer\n",
        "logit_layer = tf.layers.Dense(units=NUM_CLASSES)\n",
        "y_ = logit_layer(h2)\n",
        "\n",
        "# eval for testing phase\n",
        "correct = tf.reduce_sum(tf.cast(tf.nn.in_top_k(y_, y, 1), tf.int32))\n",
        "\n",
        "# cross entropy loss\n",
        "loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n",
        "\n",
        "# Gradient descent optimizer\n",
        "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
        "train_op = optimizer.minimize(loss)\n",
        "\n",
        "# Runtime of tensorflow: Session\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  # Epoch loop\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print('-'*20 + ' Epoch #%d ' % epoch + '-'*20)\n",
        "    sess.run(train_iter.initializer)\n",
        "    losses, steps = [], 0\n",
        "    # Process batches\n",
        "    while True:\n",
        "      try:\n",
        "        train_x, train_y = sess.run((train_nx, train_ny))\n",
        "        _, l = sess.run((train_op, loss), feed_dict={x: train_x, y: train_y})\n",
        "        losses.append(l)\n",
        "        steps += 1\n",
        "      except tf.errors.OutOfRangeError:\n",
        "        avg_loss = sum(losses) / len(losses)\n",
        "        print(\"TRAIN: batch size: %d, steps: %d, avg loss: %f.\" % (BATCH_SIZE, steps, avg_loss))\n",
        "        evalTestData(sess)\n",
        "        break\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Epoch #0 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 2.014851.\n",
            "TEST: batch size: 100, steps 100, avg loss 1.896175, precision: 68.620000\n",
            "-------------------- Epoch #1 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 1.025673.\n",
            "TEST: batch size: 100, steps 100, avg loss 1.020274, precision: 74.990000\n",
            "-------------------- Epoch #2 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.883755.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.884142, precision: 77.120000\n",
            "-------------------- Epoch #3 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.797388.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.800991, precision: 78.890000\n",
            "-------------------- Epoch #4 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.733704.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.740849, precision: 79.400000\n",
            "-------------------- Epoch #5 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.681423.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.688967, precision: 80.220000\n",
            "-------------------- Epoch #6 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.638319.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.645743, precision: 80.550000\n",
            "-------------------- Epoch #7 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.599155.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.606545, precision: 81.020000\n",
            "-------------------- Epoch #8 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.557009.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.564284, precision: 81.800000\n",
            "-------------------- Epoch #9 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.506819.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.514763, precision: 83.290000\n",
            "-------------------- Epoch #10 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.455604.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.462928, precision: 86.060000\n",
            "-------------------- Epoch #11 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.404266.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.414472, precision: 87.960000\n",
            "-------------------- Epoch #12 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.363207.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.374654, precision: 88.860000\n",
            "-------------------- Epoch #13 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.332986.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.344300, precision: 89.530000\n",
            "-------------------- Epoch #14 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.308809.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.320975, precision: 90.060000\n",
            "-------------------- Epoch #15 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.289025.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.301854, precision: 90.520000\n",
            "-------------------- Epoch #16 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.272841.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.286344, precision: 90.870000\n",
            "-------------------- Epoch #17 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.258720.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.272493, precision: 91.130000\n",
            "-------------------- Epoch #18 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.245923.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.260247, precision: 91.380000\n",
            "-------------------- Epoch #19 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.235107.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.250250, precision: 91.430000\n",
            "-------------------- Epoch #20 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.225829.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.241400, precision: 91.590000\n",
            "-------------------- Epoch #21 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.217483.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.233720, precision: 91.730000\n",
            "-------------------- Epoch #22 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.210179.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.226676, precision: 91.850000\n",
            "-------------------- Epoch #23 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.203266.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.220931, precision: 91.930000\n",
            "-------------------- Epoch #24 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.197007.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.214578, precision: 92.120000\n",
            "-------------------- Epoch #25 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.191822.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.209651, precision: 92.200000\n",
            "-------------------- Epoch #26 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.186306.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.204358, precision: 92.300000\n",
            "-------------------- Epoch #27 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.181691.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.199546, precision: 92.500000\n",
            "-------------------- Epoch #28 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.177302.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.195448, precision: 92.510000\n",
            "-------------------- Epoch #29 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.173010.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.191213, precision: 92.670000\n",
            "-------------------- Epoch #30 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.169209.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.187781, precision: 92.760000\n",
            "-------------------- Epoch #31 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.165071.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.184026, precision: 92.900000\n",
            "-------------------- Epoch #32 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.161174.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.180463, precision: 92.900000\n",
            "-------------------- Epoch #33 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.157482.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.177034, precision: 93.010000\n",
            "-------------------- Epoch #34 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.154329.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.174078, precision: 93.040000\n",
            "-------------------- Epoch #35 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.151199.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.170989, precision: 93.060000\n",
            "-------------------- Epoch #36 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.148239.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.168370, precision: 93.080000\n",
            "-------------------- Epoch #37 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.145304.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.165828, precision: 93.100000\n",
            "-------------------- Epoch #38 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.142285.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.163019, precision: 93.180000\n",
            "-------------------- Epoch #39 --------------------\n",
            "TRAIN: batch size: 100, steps: 600, avg loss: 0.139671.\n",
            "TEST: batch size: 100, steps 100, avg loss 0.160523, precision: 93.270000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7ujbEobMSCJZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}